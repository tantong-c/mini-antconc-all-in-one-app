import streamlit as st
import pandas as pd
from collections import Counter
import re
import io

# üìå ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ (PyThaiNLP)
from pythainlp import word_tokenize

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏à
st.set_page_config(page_title="Mini AntConc All-in-One",
                   layout="wide", page_icon="üêú")

# --- üé® Theme Settings ---
st.markdown("""
<style>
    .stApp { background-color: #F8F9FA; color: #212529; }
    [data-testid="stSidebar"] { background-color: #E9ECEF; }
    th { background-color: #DEE2E6 !important; color: #212529 !important; }
    .search-help { background-color: #ffffff; padding: 15px; border-radius: 5px; border: 1px solid #ddd; margin-bottom: 15px; }
</style>
""", unsafe_allow_html=True)

# --- üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏Å‡∏£‡∏≠‡∏á | ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏¥‡πâ‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô) ---


@st.cache_data(show_spinner=False)
def process_corpus(uploaded_files, auto_tokenize=True):
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå -> (‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥) -> ‡∏Å‡∏£‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ | ‡∏ó‡∏¥‡πâ‡∏á -> ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    files_data = []
    all_tokens_flat = []

    for uploaded_file in uploaded_files:
        string_data = uploaded_file.read().decode("utf-8")

        tokens = []
        display_text = ""

        if auto_tokenize:
            # ‚úÖ ‡∏Å‡∏£‡∏ì‡∏µ 1: Auto Tokenize
            raw_tokens = word_tokenize(
                string_data, engine='newmm', keep_whitespace=False)
        else:
            # ‚úÖ ‡∏Å‡∏£‡∏ì‡∏µ 2: Manual (‡∏°‡∏µ | ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß)
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà Newline ‡∏î‡πâ‡∏ß‡∏¢ | ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ split
            clean_text = string_data.replace("\n", "|")
            raw_tokens = clean_text.split("|")

        # üßπ Cleaning Step (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç):
        # 1. strip() : ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á
        # 2. if t.strip() != "" : ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏á
        # 3. if t.strip() != "|" : ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ pipe (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!)
        tokens = [
            t.strip() for t in raw_tokens
            if t.strip() != "" and t.strip() != "|"
        ]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• (File Content)
        display_text = "|".join(tokens)

        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        files_data.append({
            "filename": uploaded_file.name,
            "tokens": tokens,
            "text": display_text
        })
        all_tokens_flat.extend(tokens)

    return files_data, all_tokens_flat

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Search Helper ---


def check_token_match(token, pattern):
    token = token.lower()
    pattern = pattern.lower()

    # 4. *‡∏Ñ‡∏≥* (Strict Contains)
    if pattern.startswith("*") and pattern.endswith("*") and len(pattern) > 2:
        clean_pat = pattern[1:-1]
        return clean_pat in token[1:-1]

    # 2. *‡∏Ñ‡∏≥ (Strict Ends with)
    elif pattern.startswith("*") and len(pattern) > 1:
        clean_pat = pattern[1:]
        return token.endswith(clean_pat) and len(token) > len(clean_pat)

    # 3. ‡∏Ñ‡∏≥* (Strict Starts with)
    elif pattern.endswith("*") and len(pattern) > 1:
        clean_pat = pattern[:-1]
        return token.startswith(clean_pat) and len(token) > len(clean_pat)

    # 1. ‡∏Ñ‡∏≥ (Exact match)
    else:
        return token == pattern


def parse_search_query(query):
    query = query.strip()
    gap_pattern = re.search(r'^(\S+)\s+<(\d+)(?:-(\d+))?>\s+(\S+)$', query)
    sequence_pattern = re.search(r'^(\S+)\s+(\S+)$', query)

    if gap_pattern:
        start_word = gap_pattern.group(1)
        min_gap = int(gap_pattern.group(2))
        max_gap = int(gap_pattern.group(
            3)) if gap_pattern.group(3) else min_gap
        end_word = gap_pattern.group(4)
        return "gap", (start_word, min_gap, max_gap, end_word)
    elif sequence_pattern:
        return "gap", (sequence_pattern.group(1), 0, 0, sequence_pattern.group(2))
    else:
        return "single", query

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Analysis Core ---


def generate_kwic(files_data, keyword, window_size=7):
    results = []
    search_type, search_params = parse_search_query(keyword)

    for file_info in files_data:
        filename = file_info['filename']
        tokens = file_info['tokens']
        len_tokens = len(tokens)
        i = 0
        while i < len_tokens:
            match_found = False
            match_start = i
            match_end = i

            if search_type == "single":
                if check_token_match(tokens[i], search_params):
                    match_found = True
                    match_end = i
            elif search_type == "gap":
                start_pat, min_gap, max_gap, end_pat = search_params
                if check_token_match(tokens[i], start_pat):
                    s_range = i + 1 + min_gap
                    e_range = min(i + 1 + max_gap + 1, len_tokens)
                    for j in range(s_range, e_range):
                        if check_token_match(tokens[j], end_pat):
                            match_found = True
                            match_end = j
                            break

            if match_found:
                left = tokens[max(0, match_start - window_size):match_start]
                node = tokens[match_start:match_end+1]
                right = tokens[match_end +
                               1:min(len_tokens, match_end + window_size + 1)]
                results.append({
                    "Left": " ".join(left),
                    "Node": " ".join(node),
                    "Right": " ".join(right),
                    "File": filename
                })
            i += 1
    return pd.DataFrame(results)


def generate_ngrams(tokens, n=2, min_freq=1):
    if len(tokens) < n:
        return pd.DataFrame()
    ngrams = zip(*[tokens[i:] for i in range(n)])
    counts = Counter([" ".join(ngram) for ngram in ngrams])
    df = pd.DataFrame(counts.items(), columns=['Cluster', 'Frequency'])
    return df[df['Frequency'] >= min_freq].sort_values(by='Frequency', ascending=False).reset_index(drop=True)


def convert_df_to_csv(df):
    return df.to_csv(index=False).encode('utf-8')

# --- üñ•Ô∏è UI Section ---


st.title("üêú Mini AntConc All-in-One")
st.caption("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏î‡∏¥‡∏ö -> ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ -> ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•")
st.caption("Upload Raw Files -> Auto-segment -> Analyze Results")

with st.sidebar:
    st.header("üìÇ Upload & Settings")

    use_auto_tokenize = st.checkbox("‡πÉ‡∏´‡πâ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ (Auto Tokenize)",
                                    value=True, help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏™‡πà |")

    uploaded_files = st.file_uploader(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Text (UTF-8)", type=['txt'], accept_multiple_files=True)

    if use_auto_tokenize:
        st.info("‚ÑπÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥: PyThaiNLP (newmm)")
    else:
        st.warning("‚ö†Ô∏è ‡πÇ‡∏´‡∏°‡∏î Manual: ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ | ‡∏Ñ‡∏±‡πà‡∏ô‡∏Ñ‡∏≥‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß")

    st.link_button("Thai Word Segmenter App",
                   "https://thai-word-seg-app-rhvzfn7jkxytwlqydi8idq.streamlit.app/")

if uploaded_files:
    with st.spinner('‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ (Tokenizing)...'):
        files_data, all_tokens_flat = process_corpus(
            uploaded_files, auto_tokenize=use_auto_tokenize)

    st.success(
        f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! {len(files_data)} ‡πÑ‡∏ü‡∏•‡πå | {len(all_tokens_flat):,} ‡∏Ñ‡∏≥")

    tab1, tab2, tab3, tab4 = st.tabs(
        ["üîç Concordance", "üìä Word List", "üîó N-Grams", "üìÑ File Content"])

    # --- Tab 1: KWIC ---
    with tab1:
        st.subheader("Concordance Tool")
        with st.expander("‚ÑπÔ∏è ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Search Syntax Guidelines) - ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô"):
            st.markdown("""
            **‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:**
            
            1.  **‡∏£‡∏±‡∏Å** : ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏£‡∏±‡∏Å" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (Exact match)
            2.  **\*‡∏£‡∏±‡∏Å** : ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà **‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢** ‡∏î‡πâ‡∏ß‡∏¢ "‡∏£‡∏±‡∏Å" (‡πÄ‡∏ä‡πà‡∏ô ‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å)
            3.  **‡∏£‡∏±‡∏Å\*** : ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà **‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô** ‡∏î‡πâ‡∏ß‡∏¢ "‡∏£‡∏±‡∏Å" (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏±‡∏Å‡∏©‡∏≤, ‡∏£‡∏±‡∏Å‡πÉ‡∏Ñ‡∏£‡πà)
            4.  **\*‡∏£‡∏±‡∏Å\*** : ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ "‡∏£‡∏±‡∏Å" **‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ñ‡∏≥** (‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå)
            5.  **‡∏£‡∏±‡∏Å ‡∏°‡∏≤‡∏Å** : ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏£‡∏±‡∏Å" ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ "‡∏°‡∏≤‡∏Å"
            6.  **‡∏£‡∏±‡∏Å <3> ‡∏°‡∏≤‡∏Å** : "‡∏£‡∏±‡∏Å" ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏≠‡∏∑‡πà‡∏ô **3 ‡∏Ñ‡∏≥** ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ "‡∏°‡∏≤‡∏Å"
            7.  **‡∏£‡∏±‡∏Å <0-3> ‡∏°‡∏≤‡∏Å** : "‡∏£‡∏±‡∏Å" ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏≠‡∏∑‡πà‡∏ô **0 ‡∏ñ‡∏∂‡∏á 3 ‡∏Ñ‡∏≥** ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ "‡∏°‡∏≤‡∏Å"
            """)

        c1, c2 = st.columns([3, 1])
        search_term = c1.text_input("‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ KWIC (Keyword in Context):", "")
        window = c2.slider("‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Context Span):", 3, 20, 8)

        if search_term:
            df = generate_kwic(files_data, search_term, window)
            if not df.empty:
                df.index += 1
                st.write(f"‡∏û‡∏ö: {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                st.download_button("üì• CSV", convert_df_to_csv(
                    df), f"kwic_{search_term}.csv", "text/csv")
                st.dataframe(df, use_container_width=True, column_config={"Left": st.column_config.TextColumn(
                    width="medium"), "Node": st.column_config.TextColumn(width="small"), "Right": st.column_config.TextColumn(width="medium")})
            else:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")

    # --- Tab 2: Word List ---
    with tab2:
        st.subheader("Word List")
        wc = Counter(all_tokens_flat)
        df_wl = pd.DataFrame(wc.items(), columns=['Word', 'Frequency']).sort_values(
            'Frequency', ascending=False).reset_index(drop=True)
        df_wl.index += 1
        st.write(f"‡∏û‡∏ö: {len(wc)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        st.download_button("üì• CSV", convert_df_to_csv(
            df_wl), "wordlist.csv", "text/csv")
        st.dataframe(df_wl, use_container_width=True)

    # --- Tab 3: N-Grams ---
    with tab3:
        st.subheader("N-Grams")
        c1, c2 = st.columns(2)
        n_size = c1.number_input("N-gram size", 2, 5, 2)
        min_f = c2.number_input("Min Frequency", 1, 100, 2)

        if st.button("Start N-Grams"):
            df_ng = generate_ngrams(all_tokens_flat, n_size, min_f)
            if not df_ng.empty:
                df_ng.index += 1
                st.write(f"‡∏û‡∏ö: {len(df_ng)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                st.download_button("üì• CSV", convert_df_to_csv(
                    df_ng), f"{n_size}grams.csv", "text/csv")
                st.dataframe(df_ng, use_container_width=True)
            else:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

    # --- Tab 4: Content ---
    with tab4:
        st.subheader("‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡πâ‡∏ß")
        sel_f = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå:", [f['filename'] for f in files_data])
        txt = next((i['text']
                   for i in files_data if i['filename'] == sel_f), "")
        st.text_area("Content (Tokenized view):", txt, height=400)
        st.caption("*‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ | ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥")

else:
    st.info("üëà ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Text ‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
